{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9451aa7d-0d18-4d8d-bda1-e91d60d30985",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SBG Preprocess Application Notebook\n",
    "\n",
    "This notebook is a wrapper to the pre-process code available at https://github.com/sister-jpl/sister-preprocess. the repo is not \"installable\" so we had to clone it into this environment. while the original preprocess command has some asusmptions about files and run configs built in, we are removing that in favor of the application notebook staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1251c9f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4\n",
    "import pathlib\n",
    "\n",
    "import json\n",
    "# stage_in packages\n",
    "from unity_sds_client.resources.collection import Collection\n",
    "\n",
    "# stage_out packages\n",
    "from datetime import datetime, timezone\n",
    "from unity_sds_client.resources.dataset import Dataset\n",
    "from unity_sds_client.resources.data_file import DataFile\n",
    "\n",
    "# SISTER methods\n",
    "import glob\n",
    "import sys\n",
    "import hytools as ht\n",
    "from hytools.io import parse_envi_header\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sister.sensors import emit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ed576-cb3b-4428-b18c-ddc456ceead7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Inputs and Configurations\n",
    "\n",
    "In the original pre-process, inputs are supplied by a run_config file. This consists of 2 entries (a raw_data file, and a CRID). The system in reality needs 3 inputs files (an observation file, a radiance file, and the crid configurable.\n",
    "\n",
    "In the Unity system, the data files required will be staged in for the applicaiton, and the crid is a config item that is passed in. To make this work in Unity, we will also pass in an \"output collection\" which is needed if we want to \"persist\" the output products in the data catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a07e7e7-2ddf-46ac-9efd-000f86da7476",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# The defaults used here generally relflect a local or jupyter environment; they are replaced with \"runtime\" values when run in the system.\n",
    "input_stac_collection_file = '/unity/ads/input_collections/EMIT_L1B/catalog.json' # type: stage-in\n",
    "output_stac_catalog_dir    = '/unity/ads/outputs/SBG-L1B-PRE/process_results'                    # type: stage-out\n",
    "\n",
    "# pre-process variables\n",
    "output_collection=\"L1B_processed\"\n",
    "crid = \"001\"\n",
    "temp_directory = \"/unity/ads/temp/nb_l1b_preprocess\"\n",
    "sensor = \"EMIT\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7fa38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Import Files from STAC Item Collection\n",
    "\n",
    "Load filenames from the stage_in STAC item collection file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a09d57c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/unity/ads/input_collections/EMIT_L1B/./EMIT_L1B_RAD_001_20231206T160939_2334011_006.nc',\n",
       " '/unity/ads/input_collections/EMIT_L1B/./EMIT_L1B_OBS_001_20231206T160939_2334011_006.nc']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_collection = Collection.from_stac(input_stac_collection_file)\n",
    "data_filenames = inp_collection.data_locations()\n",
    "\n",
    "data_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8746f-ccd9-4374-aa07-d1f6a0c827f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Get the data files from the STAC files\n",
    "STAC makes no guarantee about keynames, so we need to look at the files themselves we are expecting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ddc7216-1199-4ca2-a392-7daea56a24d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBS:/unity/ads/input_collections/EMIT_L1B/./EMIT_L1B_OBS_001_20231206T160939_2334011_006.nc\n",
      "RAD:/unity/ads/input_collections/EMIT_L1B/./EMIT_L1B_RAD_001_20231206T160939_2334011_006.nc\n"
     ]
    }
   ],
   "source": [
    "for f in data_filenames:\n",
    "    if \"RAD\" in f:\n",
    "        radiance_file = f\n",
    "    elif \"OBS\" in f:\n",
    "        observation_file = f\n",
    "\n",
    "print(\"OBS:\" + observation_file)\n",
    "print(\"RAD:\" + radiance_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf66e035-4fd0-4531-a9a7-d3a75f573516",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check to see if output/temp directories exist:\n",
    "# This is really onl required if running through the notebook; want to make sure we've got the locations setup\n",
    "# for temp and output creation.\n",
    "pathlib.Path(output_stac_catalog_dir).mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path(temp_directory).mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3f0ec6-13f3-4b80-963b-ded7f9de96cd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Misc. function required by the preprocess command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee32bc59-4ed6-426c-929f-ff17835b95e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_quicklook(input_file,output_dir):\n",
    "\n",
    "    img = ht.HyTools()\n",
    "    img.read_file(input_file)\n",
    "    image_file =f\"{output_dir}/{img.base_name}.png\"\n",
    "\n",
    "    if 'DESIS' in img.base_name:\n",
    "        band3 = img.get_wave(560)\n",
    "        band2 = img.get_wave(850)\n",
    "        band1 = img.get_wave(660)\n",
    "    else:\n",
    "        band3 = img.get_wave(560)\n",
    "        band2 = img.get_wave(850)\n",
    "        band1 = img.get_wave(1660)\n",
    "\n",
    "    rgb=  np.stack([band1,band2,band3])\n",
    "    rgb[rgb == img.no_data] = np.nan\n",
    "\n",
    "    rgb = np.moveaxis(rgb,0,-1).astype(float)\n",
    "    bottom = np.nanpercentile(rgb,5,axis = (0,1))\n",
    "    top = np.nanpercentile(rgb,95,axis = (0,1))\n",
    "    rgb = np.clip(rgb,bottom,top)\n",
    "    rgb = (rgb-np.nanmin(rgb,axis=(0,1)))/(np.nanmax(rgb,axis= (0,1))-np.nanmin(rgb,axis= (0,1)))\n",
    "    rgb = (rgb*255).astype(np.uint8)\n",
    "\n",
    "    im = Image.fromarray(rgb)\n",
    "    im.save(image_file)\n",
    "\n",
    "def generate_metadata(header_file,output_dir):\n",
    "\n",
    "    header = parse_envi_header(header_file)\n",
    "    base_name =os.path.basename(header_file)[:-4]\n",
    "\n",
    "    metadata = {}\n",
    "    metadata['sensor'] = header['sensor type'].upper()\n",
    "    metadata['start_time'] = header['start acquisition time'].upper()\n",
    "    metadata['end_time'] = header['end acquisition time'].upper()\n",
    "    metadata['description'] = header['description'].capitalize()\n",
    "\n",
    "    # Split corner coordinates string into list\n",
    "    coords = [float(x) for x in header['bounding box'].replace(']','').replace('[','').split(',')]\n",
    "\n",
    "    metadata['bounding_box'] = [list(x) for x in zip(coords[::2],coords[1::2])]\n",
    "    metadata['product'] = base_name.split('_')[4]\n",
    "    metadata['processing_level'] = base_name.split('_')[2]\n",
    "\n",
    "    config_file = f'{output_dir}/{base_name}.met.json'\n",
    "\n",
    "    with open(config_file, 'w') as outfile:\n",
    "        json.dump(metadata,outfile,indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e97444",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Preprocess Command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5854bd9-4fd1-47da-b6af-741650b590f8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting EMIT L1B_RDN dataset\n",
      "Projecting data to UTM Zone 14 North at 60m resolution\n",
      "Creating output file that is 2015P x 1973L.\n",
      "Processing /unity/ads/temp/nb_l1b_preprocess/data_gcs [1/1] : 0Using internal nodata values (e.g. -9999) for image /unity/ads/temp/nb_l1b_preprocess/data_gcs.\n",
      "Copying nodata values from source /unity/ads/temp/nb_l1b_preprocess/data_gcs to destination /unity/ads/outputs/SBG-L1B-PRE/process_results/SISTER_EMIT_L1B_RDN_20231206T160939_001.bin.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Exporting EMIT location dataset\n",
      "Projecting location datacube to UTM Zone 14 North at 60m resolution\n",
      "Creating output file that is 2015P x 1973L.\n",
      "Processing /unity/ads/temp/nb_l1b_preprocess/loc_gcs [1/1] : 0Using internal nodata values (e.g. -9999) for image /unity/ads/temp/nb_l1b_preprocess/loc_gcs.\n",
      "Copying nodata values from source /unity/ads/temp/nb_l1b_preprocess/loc_gcs to destination /unity/ads/outputs/SBG-L1B-PRE/process_results/SISTER_EMIT_L1B_RDN_20231206T160939_001_LOC.bin.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n",
      "Exporting EMIT observation dataset\n",
      "Projecting observation datacube to UTM Zone 14 North at 60m resolution\n",
      "Creating output file that is 2015P x 1973L.\n",
      "Processing /unity/ads/temp/nb_l1b_preprocess/obs_gcs [1/1] : 0Using internal nodata values (e.g. -9999) for image /unity/ads/temp/nb_l1b_preprocess/obs_gcs.\n",
      "Copying nodata values from source /unity/ads/temp/nb_l1b_preprocess/obs_gcs to destination /unity/ads/outputs/SBG-L1B-PRE/process_results/SISTER_EMIT_L1B_RDN_20231206T160939_001_OBS.bin.\n",
      "...10...20...30...40...50...60...70...80...90...100 - done.\n"
     ]
    }
   ],
   "source": [
    "# This is the code we're actually interested in\n",
    "emit.nc_to_envi(radiance_file,\n",
    "                    output_stac_catalog_dir,\n",
    "                    temp_directory,\n",
    "                    obs_file = observation_file,\n",
    "                    export_loc = True,\n",
    "                    crid = crid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc58e8c-94e5-4d90-9d68-c25c2149c4dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split after the nc_to_envi\n",
    "for dataset in glob.glob(output_stac_catalog_dir+\"/SISTER*.bin\"):\n",
    "        generate_metadata(dataset.replace('.bin','.hdr'),\n",
    "                                  output_stac_catalog_dir)\n",
    "\n",
    "#Update crid\n",
    "for file in glob.glob(output_stac_catalog_dir+\"/SISTER*\"):\n",
    "    os.rename(file,file.replace('CRID',\n",
    "                                    crid))\n",
    "\n",
    "rdn_file =  glob.glob(output_stac_catalog_dir+\"/*%s.bin\" % crid)[0]\n",
    "generate_quicklook(rdn_file,output_stac_catalog_dir)\n",
    "\n",
    "# Unity does not generate thse, so we comment them out for now\n",
    "# if os.path.exists(run_config_json):\n",
    "#     shutil.copyfile(run_config_json,\n",
    "#                 output+'/%s.runconfig.json' % os.path.basename(rdn_file)[:-4])\n",
    "# #added this check because i don't think this script is creating a run.log... not sure where this comes from\n",
    "# if os.path.exists('run.log'):\n",
    "#     shutil.copyfile('run.log',\n",
    "#                 output+'/%s.log' % os.path.basename(rdn_file)[:-4])\n",
    "# done being \"interested in\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89224c4e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Create stage-out item catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b4aa5d3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# stage_in packages\n",
    "from unity_sds_client.resources.collection import Collection\n",
    "\n",
    "# stage_out packages\n",
    "from datetime import datetime, timezone\n",
    "from unity_sds_client.resources.dataset import Dataset\n",
    "from unity_sds_client.resources.data_file import DataFile\n",
    "\n",
    "# Create a collection\n",
    "out_collection = Collection(output_collection)\n",
    "\n",
    "data_files = glob.glob(output_stac_catalog_dir+\"/SISTER*RDN*.bin\") \n",
    "# hack to get the radiance file\n",
    "data_file = os.path.basename(data_files[0].replace(\"_LOC\",\"\").replace(\"_OBS\",\"\"))\n",
    "name=os.path.splitext(data_file)[0]\n",
    "\n",
    "# Get some metadata from met.json file\n",
    "with open(output_stac_catalog_dir + \"/\" + name+\".met.json\") as metadata:\n",
    "    md_dict = json.load(metadata)\n",
    "    start_time = md_dict['start_time']\n",
    "    end_time = md_dict['end_time']\n",
    "\n",
    "# Create a Dataset for the collection\n",
    "dataset = Dataset(\n",
    "    name=name, \n",
    "    collection_id=out_collection.collection_id, \n",
    "    start_time=start_time, \n",
    "    end_time=end_time,\n",
    "    creation_time=datetime.utcnow().replace(tzinfo=timezone.utc).isoformat(),\n",
    ")\n",
    "\n",
    "# Add output file(s) to the dataset\n",
    "for file in glob.glob(output_stac_catalog_dir+\"/SISTER*\"):\n",
    "    #type, location, roles = [], title = \"\", description = \"\" \n",
    "    if file.endswith(\".bin\"):\n",
    "        dataset.add_data_file(DataFile(\"binary\",file, [\"data\"]))\n",
    "    elif file.endswith(\".png\"):\n",
    "        dataset.add_data_file(DataFile(\"image/png\",file, [\"browse\"]))\n",
    "    else:\n",
    "        dataset.add_data_file(DataFile(None,file, [\"metadata\"]))\n",
    "        \n",
    "#Add the STAC file we are creating\n",
    "\n",
    "# the future metadata file needs to be added to the STAC as well\n",
    "    # will eventually be moved into the to_stac() function\n",
    "dataset.add_data_file(DataFile(\"text/json\",os.path.join(output_stac_catalog_dir, name + \".json\"), [\"metadata\"]))\n",
    "\n",
    "# Add the dataset to the collection\n",
    "#out_collection.add_dataset(dataset)\n",
    "out_collection._datasets.append(dataset)\n",
    "\n",
    "Collection.to_stac(out_collection, output_stac_catalog_dir)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
